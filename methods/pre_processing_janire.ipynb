{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import requests \n",
    "import zipfile\n",
    "\n",
    "\n",
    "DATA_FOLDER = \"data\"\n",
    "DATA_RAW = os.path.join(DATA_FOLDER, \"raw\")\n",
    "DATA_PROCESSED=os.path.join(DATA_FOLDER, \"processed\")\n",
    "DATASET_URL = \"https://storage.googleapis.com/tecla/spotify-million-playlist-dataset/spotify_million_playlist_dataset.zip\"  \n",
    "TRACKS_DF_FILENAME = \"tracks_df.csv\"\n",
    "PLAYLISTS_DF_FILENAME = \"playlists_df.csv\"\n",
    "PLAYLIST_TRACKS_DF_FILENAME = \"playlist_tracks_df.csv\"\n",
    "\n",
    "\n",
    "playlists_df_list, tracks_df_list = [], []\n",
    "track_uri_to_id = {}\n",
    "\n",
    "\n",
    "def validate_dict(dictionary, expected_keys, expected_types):\n",
    "    '''\n",
    "    Given a dictionary, test if it contains all the expected keys and\n",
    "    with expected type of values\n",
    "\n",
    "    Args:\n",
    "        dictionary(dict): The dictionary to be tested\n",
    "        expected_keys(tuple): The expected keys\n",
    "        expected_types(tuple): The expected value types for each key\n",
    "    Returns:\n",
    "        tuple(bool, str): whether the test passed, the reason if failed\n",
    "    '''\n",
    "    assert isinstance(dictionary, dict)\n",
    "\n",
    "    for expected_key, expected_type in zip(expected_keys, expected_types):\n",
    "        if expected_key not in dictionary:\n",
    "            return False, f\"{expected_key} not in dictionary\"\n",
    "        if not isinstance(dictionary[expected_key], expected_type):\n",
    "            return False, f\"dictionary[{expected_key}] is not a {expected_type}\"\n",
    "\n",
    "    return True, None\n",
    "\n",
    "def process_slice(slice):\n",
    "    '''\n",
    "    Given a slice with data structure described in \"Raw Data Structure.png\",\n",
    "    modify this slice by\n",
    "    1. Removing the \"info\" field\n",
    "    2. Adding the attribute \"slice\"(originally in \"info\") to the slice\n",
    "    3. In an entry of \"playlists\",\n",
    "       convert \"collaborative\" from str to bool, and\n",
    "       convert \"duration_ms\" from ms to secs(int)\n",
    "    4. In an entry of \"tracks\" in an entry of \"playlists\",\n",
    "       convert \"duration_ms\" from ms to secs(int)\n",
    "    *Check \"New Data Structure.png\" for more details*\n",
    "    Then, add all of the playlists and tracks into the dataframes\n",
    "\n",
    "    Args:\n",
    "        slice(dict): a slice to be processed\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    assert isinstance(slice, list)\n",
    "    res = validate_slice(slice)\n",
    "    assert res[0], res[1]\n",
    "\n",
    "    # # removing the info field and bringing \"slice\" toe the top level\n",
    "    # slice[\"slice\"] = slice[\"info\"][\"slice\"]\n",
    "    # slice.pop(\"info\")\n",
    "\n",
    "    for playlist in slice:\n",
    "        # convert \"collaborative\" from str to bool for each playlist\n",
    "        collab = playlist[\"collaborative\"]\n",
    "        playlist[\"collaborative\"] = (collab == \"true\")\n",
    "\n",
    "        # convert \"duration_ms\" to \"duration_s\" for each playlist\n",
    "        playlist[\"duration_s\"] = playlist[\"duration_ms\"] // 1000\n",
    "        playlist.pop(\"duration_ms\")\n",
    "\n",
    "        for track in playlist[\"tracks\"]:\n",
    "            # convert \"duration_ms\" to \"duration_s\" for each track\n",
    "            track[\"duration_s\"] = track[\"duration_ms\"] // 1000\n",
    "            track.pop(\"duration_ms\")\n",
    "\n",
    "            # new track, append it to tracks_df\n",
    "            if (len(track_uri_to_id) == 0 or\n",
    "               track[\"track_uri\"] not in track_uri_to_id):\n",
    "                track[\"track_id\"] = len(track_uri_to_id)\n",
    "                track_uri_to_id[track[\"track_uri\"]] = track[\"track_id\"]\n",
    "                del track[\"pos\"]\n",
    "                tracks_df_list.append(track)\n",
    "\n",
    "        # encode tracks to their ids\n",
    "        ids = [track_uri_to_id[track[\"track_uri\"]]\n",
    "               for track in playlist[\"tracks\"]]\n",
    "        playlist[\"tracks\"] = ids\n",
    "\n",
    "        # most playlist doesn't have a description\n",
    "        if \"description\" in playlist:\n",
    "            del playlist[\"description\"]\n",
    "        # add the playlist to playlists_df\n",
    "        playlists_df_list.append(playlist)\n",
    "\n",
    "\n",
    "def pre_process_dataset(path, new_path):\n",
    "    '''\n",
    "    Given the directory of the dataset, for each slice first modified it by\n",
    "    the rules described in generate_new_slice.\n",
    "\n",
    "    Then, generate 3 dataframes:\n",
    "    playlists_df, tracks_df, and playlist_tracks_df\n",
    "\n",
    "    playlists_df has the fields: pid, name, other metadata,\n",
    "    and tracks which is a list containig the id of each track in the playlist.\n",
    "\n",
    "    tracks_df has the fields: track_id, name, artist, and other metadata.\n",
    "\n",
    "    playlist_tracks_df has the field: track_id and pid which could be used to\n",
    "    tell which playlists contain a certain track.\n",
    "\n",
    "\n",
    "    The generated dataframe will be saved in to the new_path directory with\n",
    "    names \"playlists_df.csv\", \"tracks_df.csv\", and \"playlist_track.csv\".\n",
    "\n",
    "    Args:\n",
    "        path(str): Directory of the MPD dataset\n",
    "        new_path(str): Directory of where to store the dataframes\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    global playlists_df_list, tracks_df_list, track_uri_to_id\n",
    "    assert isinstance(path, str)\n",
    "    assert isinstance(new_path, str)\n",
    "\n",
    "    filenames = os.listdir(path)\n",
    "    # go through each file in the directory\n",
    "    for filename in tqdm(filenames):\n",
    "        # check if the file is a slice of the dataset\n",
    "        if filename.startswith(\"chunk_1\") and filename.endswith(\".parquet\"):\n",
    "            # load the slice\n",
    "            file_path = os.path.join(path, filename)\n",
    "            mpd_slice = pd.read_parquet(file_path)\n",
    "\n",
    "                # Load the first slice\n",
    "            mpd_slice = pd.read_parquet(file_path)\n",
    "            \n",
    "            mpd_slice_dicts = mpd_slice.to_dict()  # 'records' gives a list of dictionaries\n",
    "\n",
    "            # process each dictionary in the list individually\n",
    "            for mpd_slice in mpd_slice_dicts:\n",
    "                process_slice(mpd_slice)\n",
    "            \n",
    "    del track_uri_to_id\n",
    "    # generate tracks_df and playlists_df\n",
    "    if not os.path.isdir(new_path):\n",
    "        os.makedirs(new_path)\n",
    "    tracks_df = pd.DataFrame.from_dict(tracks_df_list)\n",
    "    del tracks_df_list\n",
    "    playlists_df = pd.DataFrame.from_dict(playlists_df_list)\n",
    "    del playlists_df_list\n",
    "    tracks_df.to_csv(os.sep.join((new_path, TRACKS_DF_FILENAME)), index=False)\n",
    "    playlists_df.to_csv(os.sep.join((new_path, PLAYLISTS_DF_FILENAME)),\n",
    "                        index=False)\n",
    "\n",
    "    # generate playlist_tracks_df\n",
    "    playlist_tracks_df = pd.DataFrame({\n",
    "        \"track_id\": playlists_df[\"tracks\"].explode(),\n",
    "        \"pid\": playlists_df[\"pid\"].repeat(\n",
    "            playlists_df[\"tracks\"].apply(len))\n",
    "    })\n",
    "    playlist_tracks_df.to_csv(os.sep.join((new_path,\n",
    "                                           PLAYLIST_TRACKS_DF_FILENAME)),\n",
    "                              index=False)\n",
    "\n",
    "\n",
    "def read_pre_processed_data(data_path):\n",
    "    \"\"\"Read the pre-processed MPD data into dataframes.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): A path to the directory that contains the pre-processed\n",
    "            MPD data CSVs.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of three dataframes of the respective playlists data, tracks data,\n",
    "        and playlists/tracks relations data.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError if data_path or any contained files does not exist or is invalid.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_path):\n",
    "        raise ValueError(f\"Data path {data_path} does not exist.\")\n",
    "    \n",
    "    if not os.path.isdir(data_path):\n",
    "        raise ValueError(f\"Data path {data_path} must be a directory.\")\n",
    "    \n",
    "    playlists_filename = os.path.join(data_path, PLAYLISTS_DF_FILENAME)\n",
    "    tracks_filename = os.path.join(data_path, TRACKS_DF_FILENAME)\n",
    "    playlists_tracks_filename = os.path.join(data_path, PLAYLIST_TRACKS_DF_FILENAME)\n",
    "    \n",
    "    if not os.path.isfile(playlists_filename):\n",
    "        raise ValueError(f\"Playlists filename {playlists_filename} must exist.\")\n",
    "    \n",
    "    if not os.path.isfile(tracks_filename):\n",
    "        raise ValueError(f\"Playlists filename {tracks_filename} must exist.\")\n",
    "    \n",
    "    if not os.path.isfile(playlists_tracks_filename):\n",
    "        raise ValueError(f\"Playlists filename {playlists_tracks_filename} must exist.\")\n",
    "\n",
    "    playlists_df = pd.read_csv(playlists_filename)\n",
    "    tracks_df = pd.read_csv(tracks_filename)\n",
    "    playlists_tracks_df = pd.read_csv(playlists_tracks_filename)\n",
    "\n",
    "    return playlists_df, tracks_df, playlists_tracks_df\n",
    "\n",
    "def download_and_extract_dataset(url, filepath, extract_to):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from a given URL and saves it to the specified path.\n",
    "    If the file is a zip file, it will be extracted to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the dataset to download.\n",
    "        filepath (str): Path to save the downloaded file.\n",
    "        extract_to (str): Directory to extract contents if the file is a zip.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        # Download the file in chunks to the specified path\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"Dataset downloaded successfully and saved to {filepath}\")\n",
    "\n",
    "        # Check if the downloaded file is a zip and extract if it is\n",
    "        if zipfile.is_zipfile(filepath):\n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"Dataset extracted successfully to {extract_to}\")\n",
    "        else:\n",
    "            print(\"Downloaded file is not a zip archive, no extraction performed.\")\n",
    "    else:\n",
    "        print(\"Failed to download the dataset. Please check the URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists: data\n",
      "Folder already exists: data\\raw\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create data and exploration_results folders if they don't exist\n",
    "    for folder in [DATA_FOLDER, DATA_RAW]:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            print(f\"Created folder: {folder}\")\n",
    "        else:\n",
    "            print(f\"Folder already exists: {folder}\")\n",
    "\n",
    "    # if not os.path.exists(\"{}/data\".format(DATA_RAW)):\n",
    "    #     print(f\"[INFO] Downloading the data ...\")\n",
    "    #     download_and_extract_dataset(DATASET_URL, \"data/row_data.zip\", DATA_RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_slice(slice):\n",
    "    '''\n",
    "    Given a slice, test if it has data structure desribed in\n",
    "    \"Raw Data Structure.png\"\n",
    "\n",
    "    Args:\n",
    "        slice(dict): a slice to be tested\n",
    "    Returns:\n",
    "        tuple(bool, str): whether the test passed, the reason if failed\n",
    "    '''\n",
    "\n",
    "    if not isinstance(slice, list):\n",
    "        return False, \"slice is not a dict\"\n",
    "\n",
    "    # expected_keys = ()\n",
    "    # expected_types = (list)\n",
    "    # res = validate_dict(slice, expected_keys, expected_types)\n",
    "    # if res[0] is False:\n",
    "    #     return res\n",
    "\n",
    "    for playlist in slice:\n",
    "        playlist[\"tracks\"] = playlist[\"tracks\"].tolist()\n",
    "        expected_keys = (\"name\", \"collaborative\", \"pid\", \"modified_at\",\n",
    "                         \"num_tracks\", \"num_albums\", \"num_followers\",\n",
    "                         \"num_edits\", \"duration_ms\", \"num_artists\",\n",
    "                         \"tracks\")\n",
    "        expected_types = (str, str, int, int,\n",
    "                          int, int, int,\n",
    "                          int, int, int,\n",
    "                          list)\n",
    "        # for key, expected_type in zip(expected_keys, expected_types):\n",
    "        #     real_type = type(playlist.get(key))  # Get the type name as a string\n",
    "        #     print(f\"Key: {key}, Expected Type: {expected_type.__name__}, Real Type: {real_type}\")\n",
    "        res = validate_dict(playlist, expected_keys, expected_types)\n",
    "        if res[0] is False:\n",
    "            return res\n",
    "\n",
    "        for track in playlist[\"tracks\"]:\n",
    "            expected_keys = (\"pos\", \"artist_name\", \"track_uri\", \"artist_uri\",\n",
    "                             \"track_name\", \"album_uri\", \"duration_ms\",\n",
    "                             \"album_name\")\n",
    "            expected_types = (int, str, str, str,\n",
    "                              str, str, int,\n",
    "                              str)\n",
    "            res = validate_dict(track, expected_keys, expected_types)\n",
    "            if res[0] is False:\n",
    "                return res\n",
    "\n",
    "    return True, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janir\\Documents\\University\\M. Eng AI\\Deep Learning\\Group Project - Spotify\\mpd-music-recommender-evaluation\\data c:\\Users\\janir\\Documents\\University\\M. Eng AI\\Deep Learning\\Group Project - Spotify\\mpd-music-recommender-evaluation\\data\\processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file...\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.join(\"..\", \"data\"))\n",
    "new_path = os.path.abspath(os.path.join(\"..\", DATA_PROCESSED))\n",
    "\n",
    "print(path, new_path)\n",
    "\n",
    "global playlists_df_list, tracks_df_list, track_uri_to_id\n",
    "assert isinstance(path, str)\n",
    "assert isinstance(new_path, str)\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "# go through each file in the directory\n",
    "for filename in tqdm(filenames):\n",
    "    # check if the file is a slice of the dataset\n",
    "    if filename.startswith(\"chunk_1\") and filename.endswith(\".parquet\"):\n",
    "        # load the slice\n",
    "        file_path = os.path.join(path, filename)\n",
    "        print(\"Reading file...\")\n",
    "        mpd_slice = pd.read_parquet(file_path)\n",
    "\n",
    "        mpd_slice_dicts = mpd_slice.to_dict(orient='records') \n",
    "        print(\"Processing data...\")\n",
    "        process_slice(mpd_slice_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tracks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m playlists_df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39msep\u001b[38;5;241m.\u001b[39mjoin((new_path, PLAYLISTS_DF_FILENAME)),\n\u001b[0;32m     11\u001b[0m                     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# generate playlist_tracks_df\u001b[39;00m\n\u001b[0;32m     14\u001b[0m playlist_tracks_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mplaylists_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtracks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mexplode(),\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m\"\u001b[39m: playlists_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mrepeat(\n\u001b[0;32m     17\u001b[0m         playlists_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m))\n\u001b[0;32m     18\u001b[0m })\n\u001b[0;32m     19\u001b[0m playlist_tracks_df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39msep\u001b[38;5;241m.\u001b[39mjoin((new_path,\n\u001b[0;32m     20\u001b[0m                                         PLAYLIST_TRACKS_DF_FILENAME)),\n\u001b[0;32m     21\u001b[0m                             index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\range.py:349\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tracks'"
     ]
    }
   ],
   "source": [
    "del track_uri_to_id\n",
    "# generate tracks_df and playlists_df\n",
    "if not os.path.isdir(new_path):\n",
    "    os.makedirs(new_path)\n",
    "tracks_df = pd.DataFrame.from_dict(tracks_df_list)\n",
    "del tracks_df_list\n",
    "playlists_df = pd.DataFrame.from_dict(playlists_df_list)\n",
    "del playlists_df_list\n",
    "tracks_df.to_csv(os.sep.join((new_path, TRACKS_DF_FILENAME)), index=False)\n",
    "playlists_df.to_csv(os.sep.join((new_path, PLAYLISTS_DF_FILENAME)),\n",
    "                    index=False)\n",
    "\n",
    "# generate playlist_tracks_df\n",
    "playlist_tracks_df = pd.DataFrame({\n",
    "    \"track_id\": playlists_df[\"tracks\"].explode(),\n",
    "    \"pid\": playlists_df[\"pid\"].repeat(\n",
    "        playlists_df[\"tracks\"].apply(len))\n",
    "})\n",
    "playlist_tracks_df.to_csv(os.sep.join((new_path,\n",
    "                                        PLAYLIST_TRACKS_DF_FILENAME)),\n",
    "                            index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
